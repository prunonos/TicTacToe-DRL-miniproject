{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfMqO01MWF8y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import namedtuple, deque\n",
        "import random\n",
        "random.seed(45)\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,\"/content/drive/MyDrive/ANN\")\n",
        "\n",
        "from tic_env import TictactoeEnv, OptimalPlayer\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = \"/content/drive/MyDrive/ANN/\""
      ],
      "metadata": {
        "id": "GzsPaNEO-K_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-learning"
      ],
      "metadata": {
        "id": "dgS3KOJdcEMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = TictactoeEnv()"
      ],
      "metadata": {
        "id": "UThLAwn8qAzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QlearningPlayer:\n",
        "    '''\n",
        "    Description:\n",
        "        A class to implement a player in Tic-tac-toe based on the Q-learning algorithm\n",
        "        combined with epsilon-greedy policy.\n",
        "    \n",
        "    Parameters:\n",
        "        alpha: float, in [0, 1]. This is the learning rate that represents the \n",
        "               step length taken to update the estimation of Q. \n",
        "        epsilon: float, in [0, 1]. This is a value between 0-1 that indicates the\n",
        "                probability of making a random action instead of the action that\n",
        "                has the highest Q-value at any given time.\n",
        "        discount_factor: float, in [0, 1]. This is a value that discounts future rewards\n",
        "                         because they are less valuable than current rewards.\n",
        "    '''\n",
        "    def __init__(self, player='X',epsilon=0.05):\n",
        "        self.discount_factor = 0.99\n",
        "        self.alpha = 0.05\n",
        "        self.epsilon = epsilon # changed 0.05 per epsilon that we pass it as parameter when creating instance\n",
        "        self.player = player \n",
        "        self.player2value = {'X': 1, 'O': -1}\n",
        "        #dictionary state -> Q-value\n",
        "        self.Q_state_action = {}\n",
        "        \n",
        "    def set_player(self, player = 'O', j=-1):\n",
        "        self.player = player\n",
        "        if j != -1:\n",
        "            self.player = 'O' if j % 2 == 0 else 'X' #the opposite is done in OptimalPlayer\n",
        "\n",
        "    def empty(self, grid):\n",
        "        '''return all empty positions'''\n",
        "        avail = []\n",
        "        for i in range(9):\n",
        "            pos = (int(i/3), i % 3)\n",
        "            if grid[pos] == 0:\n",
        "                avail.append(pos)\n",
        "        return avail\n",
        "    \n",
        "    def randomMove(self, grid):\n",
        "        \"\"\" Choose a random move from the available options. \"\"\"\n",
        "        avail = self.empty(grid)\n",
        "        return avail[random.randint(0, len(avail)-1)]\n",
        "    \n",
        "    def highestQMove(self, grid):\n",
        "        \"\"\"\n",
        "        Choose the action which has the highest Q-value and is an available option.\n",
        "        Also, return the corresponding Q-value.\n",
        "        \"\"\"\n",
        "        \n",
        "        avail = self.empty(grid)\n",
        "        max_Q_value = -np.Inf\n",
        "        highest_Q_move = avail[random.randint(0, len(avail)-1)]\n",
        "        \n",
        "        for action in avail:\n",
        "            #obtain grid if this specific action is played\n",
        "            next_grid = grid.copy()\n",
        "            next_grid[action] = self.player2value[self.player]\n",
        "            hash_next_grid = self.hashGrid(next_grid) \n",
        "            \n",
        "            #obtain Q_value for this state\n",
        "            Q_value = self.Q_state_action.get(hash_next_grid, 0) #default is 0\n",
        "            \n",
        "            #update max Q_value\n",
        "            if Q_value >= max_Q_value:\n",
        "                max_Q_value = Q_value\n",
        "                highest_Q_move = action\n",
        "                \n",
        "        return max_Q_value, highest_Q_move\n",
        "    \n",
        "    def act(self, grid):\n",
        "        \"\"\"\n",
        "        A touple is returned that represents (row, col) or \n",
        "        an int corresponding to the 9 possible places on the grid.\n",
        "        \"\"\"\n",
        "        # choose action according to epsilon-greedy policy\n",
        "        # i.e. whether to move in random or not\n",
        "        if random.random() < self.epsilon: action = self.randomMove(grid)\n",
        "        else: _, action = self.highestQMove(grid)\n",
        "       \n",
        "        return action\n",
        "    \n",
        "    def hashGrid(self, grid):\n",
        "        \"\"\"Hash the grid in order to be able to use it as a dictionary key\"\"\"\n",
        "        return str(grid.reshape(3 * 3))\n",
        "    \n",
        "    def feedReward(self, reward, grid):\n",
        "        \"\"\"\n",
        "        Update Q-values when the game end, i.e. a reward is obtained. \n",
        "        \"\"\"\n",
        "        hash_grid = self.hashGrid(grid)\n",
        "        Q_final_grid_move = self.Q_state_action.get(hash_grid, 0)\n",
        "        \n",
        "        #when the game has ended we have a reward and no next state\n",
        "        self.Q_state_action[hash_grid] = Q_final_grid_move + self.alpha * (reward - Q_final_grid_move) \n",
        "        \n",
        "    def tdUpdate(self, grid_before, grid_after):\n",
        "        \"\"\"\n",
        "        Update Q-values after an action that doesn't end the game, i.e. no reward is obtained.\n",
        "        \"\"\"\n",
        "        #grid after is the grid after this player's move and after the opponent has played\n",
        "        \n",
        "        #when the game hasn't ended, we don't have a reward, but there is a next state\n",
        "        hash_grid_before = self.hashGrid(grid_before)\n",
        "        Q_grid_move = self.Q_state_action.get(hash_grid_before, 0)\n",
        "        max_Q_value, _ = self.highestQMove(grid_after)\n",
        "        \n",
        "        self.Q_state_action[hash_grid_before] = Q_grid_move + self.alpha * (self.discount_factor * max_Q_value - Q_grid_move)     "
      ],
      "metadata": {
        "id": "4MlIeIhPqBSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1"
      ],
      "metadata": {
        "id": "LDfIzDk4tefu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "player_opt = OptimalPlayer(epsilon=0.5, player = 'X')\n",
        "player_q_learn = QlearningPlayer(player = 'O')\n",
        "\n",
        "nr_games = 20000\n",
        "reward_every_250_games = 0\n",
        "average_rewards = {}\n",
        "\n",
        "\n",
        "for i in range(1, nr_games + 1):\n",
        "    \n",
        "    if i % 250 == 0: \n",
        "        average_rewards[i] = reward_every_250_games / 250\n",
        "        reward_every_250_games = 0\n",
        "    \n",
        "    env.reset()\n",
        "    \n",
        "    #switch the 1st player after every game\n",
        "    player_opt.set_player(j=i)\n",
        "    player_q_learn.set_player(j=i)\n",
        "    \n",
        "    grid, _, __ = env.observe()\n",
        "    \n",
        "    for j in range(9):\n",
        "        \n",
        "        if env.current_player == player_opt.player:\n",
        "            move = player_opt.act(grid)\n",
        "            grid_after_player_opt, end, winner = env.step(move, print_grid=False)\n",
        "            #if j=0 here, then player_opt starts the game and we don't want to update\n",
        "            #the empty grid state\n",
        "            if not end and j != 0: player_q_learn.tdUpdate(grid, grid_after_player_opt)\n",
        "        \n",
        "        else:\n",
        "            move = player_q_learn.act(grid)\n",
        "            grid_after_player_q_learn, end, winner = env.step(move, print_grid=False)\n",
        "        \n",
        "        if end:           \n",
        "            reward = env.reward(player = player_q_learn.player)\n",
        "            reward_every_250_games += reward\n",
        "            player_q_learn.feedReward(reward, grid_after_player_q_learn)\n",
        "            env.reset()\n",
        "            break\n",
        "            \n",
        "        else:\n",
        "            #current_player has changed after step()\n",
        "            if env.current_player == player_opt.player: grid = grid_after_player_q_learn\n",
        "            else: grid = grid_after_player_opt"
      ],
      "metadata": {
        "id": "adFKt2Fqruet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(list(average_rewards.keys()), list(average_rewards.values()))\n",
        "plt.title(\"Q-learning(0.05) vs Opt(0.5)\")\n",
        "plt.xlabel(\"Number of games played\")\n",
        "plt.ylabel(\"Average reward for every 250 games\")\n",
        "plt.ylim((-0.25, 1))\n",
        "plt.grid()\n",
        "plt.savefig(save_dir + \"Q-learning(0.05) vs Opt(0.5).jpg\", dpi=400)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7L7bdcMItxQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2"
      ],
      "metadata": {
        "id": "pegyjgWEv4wP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "e_max, e_min = 0.8, 0.1\n",
        "n_expl = np.linspace(1, 20000, 5, dtype=int)\n",
        "\n",
        "average_rewards_array = []\n",
        "\n",
        "for n in n_expl:\n",
        "\n",
        "    player_opt = OptimalPlayer(epsilon=0.5, player = 'X')\n",
        "    player_q_learn = QlearningPlayer(player = 'O')\n",
        "\n",
        "    nr_games = 20000\n",
        "    reward_every_250_games = 0\n",
        "    average_rewards = {}\n",
        "\n",
        "    for i in range(1, nr_games + 1):\n",
        "        \n",
        "        if i%250 == 0: \n",
        "            average_rewards[i] = reward_every_250_games / 250\n",
        "            reward_every_250_games = 0\n",
        "        \n",
        "        env.reset()\n",
        "        \n",
        "        # update epsilon\n",
        "        player_q_learn.epsilon = np.max([e_min, e_max * (1 - i / n)])\n",
        "        \n",
        "        #switch the 1st player after every game\n",
        "        player_opt.set_player(j=i)\n",
        "        player_q_learn.set_player(j=i)\n",
        "        \n",
        "        grid, _, __ = env.observe()\n",
        "            \n",
        "        for j in range(9):\n",
        "            \n",
        "            if env.current_player == player_opt.player:\n",
        "                move = player_opt.act(grid)\n",
        "                grid_after_player_opt, end, winner = env.step(move, print_grid=False)\n",
        "                \n",
        "                if not end and j!=0: player_q_learn.tdUpdate(grid, grid_after_player_opt)\n",
        "            \n",
        "            else:\n",
        "                move = player_q_learn.act(grid)\n",
        "                grid_after_player_q_learn, end, winner = env.step(move, print_grid=False)\n",
        "\n",
        "            \n",
        "            if end:           \n",
        "                reward = env.reward(player = player_q_learn.player)\n",
        "                reward_every_250_games += reward\n",
        "                player_q_learn.feedReward(reward, grid_after_player_q_learn)\n",
        "\n",
        "                env.reset()\n",
        "                \n",
        "                break\n",
        "                \n",
        "            else:\n",
        "                #current_player has changed after step\n",
        "                if env.current_player == player_opt.player: grid = grid_after_player_q_learn\n",
        "                else: grid = grid_after_player_opt\n",
        "\n",
        "    average_rewards_array.append(average_rewards)              "
      ],
      "metadata": {
        "id": "CSX9f4YTv4Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(1, figsize=(15, 10))\n",
        "for i,_ in enumerate(n_expl):\n",
        "    plt.plot(list(average_rewards_array[i].keys()), list(average_rewards_array[i].values()))\n",
        "plt.title(f\"Average reward of Q-learning agent with decreasing exploration vs Opt(0.5) for every 250 games\")\n",
        "plt.legend(n_expl,title='${n^*}$ values')\n",
        "plt.xlabel(\"Number of games\")\n",
        "plt.ylabel(\"Average reward\")\n",
        "plt.grid()\n",
        "plt.ylim((0, 1))\n",
        "plt.savefig(save_dir + \"decreasing exploration.jpg\", dpi=400)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_C-M1K_I1uyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3"
      ],
      "metadata": {
        "id": "CuCj7Y1zCLmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_game(player_q_learn, second_player, i):\n",
        "\n",
        "    env.reset()\n",
        "    second_player.set_player(j=i); player_q_learn.set_player(j=i)\n",
        "    grid, _, _ =  env.observe()\n",
        "\n",
        "    for j in range(9):\n",
        "        if env.current_player == second_player.player:\n",
        "            move = second_player.act(grid)\n",
        "        else:\n",
        "            move = player_q_learn.act(grid)\n",
        "\n",
        "        grid, end, _ = env.step(move, print_grid=False)\n",
        "\n",
        "        if end:\n",
        "            reward = env.reward(player = player_q_learn.player)\n",
        "            break\n",
        "\n",
        "    return reward"
      ],
      "metadata": {
        "id": "xxT3giYL3vN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def computeTestM(player_q_learn):\n",
        "    N = 500\n",
        "    opt, ran = 0, 0\n",
        "    player_opt = OptimalPlayer()\n",
        "    player_opt.epsilon, player_q_learn.epsilon = 0, 0\n",
        "    \n",
        "    # First we compute Mopt\n",
        "    for i in range(N):\n",
        "        opt += one_game(player_q_learn,player_opt, i)\n",
        "        \n",
        "    # Then we compute Mrand\n",
        "    player_opt.epsilon = 1\n",
        "    for i in range(N):\n",
        "        ran += one_game(player_q_learn,player_opt, i)    \n",
        "    \n",
        "    return opt / N, ran / N"
      ],
      "metadata": {
        "id": "c6cjsb9FoSSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e_max, e_min = 0.8, 0.1\n",
        "n_expl = np.linspace(1, 20000, 5, dtype=int)\n",
        "\n",
        "average_mopt_array = []\n",
        "average_mran_array = []\n",
        "\n",
        "for n in n_expl:\n",
        "  player_opt = OptimalPlayer(epsilon=0.5, player = 'X')\n",
        "  player_q_learn = QlearningPlayer(player = 'O')\n",
        "\n",
        "  nr_games = 20000\n",
        "\n",
        "  average_mopt = {}\n",
        "  average_mran = {}\n",
        "\n",
        "  for i in range(1, nr_games + 1):\n",
        "      \n",
        "      if i%250 == 0: \n",
        "          average_mopt[i], average_mran[i] = computeTestM(player_q_learn)\n",
        "      \n",
        "      env.reset()\n",
        "      player_q_learn.epsilon = np.max([e_min, e_max*(1 - i / n)])\n",
        "      player_opt.set_player(j=i)\n",
        "      player_q_learn.set_player(j=i)\n",
        "      grid, _, _ = env.observe()\n",
        "\n",
        "      for j in range(9):\n",
        "              if env.current_player == player_opt.player:\n",
        "                  move = player_opt.act(grid)\n",
        "                  grid_after_player_opt, end, winner = env.step(move, print_grid=False)\n",
        "                  if not end and j != 0: player_q_learn.tdUpdate(grid, grid_after_player_opt)\n",
        "              else:\n",
        "                  move = player_q_learn.act(grid)\n",
        "                  grid_after_player_q_learn, end, winner = env.step(move, print_grid=False)\n",
        "              if end:           \n",
        "                  reward = env.reward(player = player_q_learn.player)\n",
        "                  reward_every_250_games += reward\n",
        "                  player_q_learn.feedReward(reward, grid_after_player_q_learn)\n",
        "                  break\n",
        "              else:\n",
        "                  if env.current_player == player_opt.player: grid = grid_after_player_q_learn\n",
        "                  else: grid = grid_after_player_opt\n",
        "  average_mopt_array.append(average_mopt)\n",
        "  average_mran_array.append(average_mran)"
      ],
      "metadata": {
        "id": "SUDIPws6oT5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(1, figsize=(15, 10))\n",
        "colors = ['orange', 'purple', 'green','red', 'pink']\n",
        "for i,n in enumerate(n_expl):\n",
        "    plt.plot(list(average_mopt_array[i].keys()), list(average_mopt_array[i].values()), label=n, linestyle='-', color=colors[i])\n",
        "    plt.plot(list(average_mran_array[i].keys()), list(average_mran_array[i].values()), label=n, linestyle='--', color=colors[i])\n",
        "plt.title(r\"$M_{rand}$ (--) and $M_{opt}$ (-) for different values of $n^*$\")\n",
        "plt.legend(title='${n^*}$ values')\n",
        "plt.xlabel(\"Number of games\")\n",
        "plt.ylabel('Mean reward')\n",
        "plt.grid()\n",
        "plt.savefig(save_dir + \"M_opt and M_rand n*.jpg\", dpi=400)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mbsl8PRBsV6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4"
      ],
      "metadata": {
        "id": "wsNDDo46tdWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nr_games = 20000\n",
        "n = 5000\n",
        "e_max, e_min = 0.8, 0.1\n",
        "eps_opt = np.linspace(0,1,5) \n",
        "average_mopt_array = []\n",
        "average_mran_array = []\n",
        "\n",
        "for e_opt in eps_opt:\n",
        "\n",
        "    player_opt = OptimalPlayer(epsilon=e_opt, player='X')\n",
        "    player_q_learn = QlearningPlayer(player='O')\n",
        "\n",
        "    average_mopt = {}\n",
        "    average_mran = {}\n",
        "\n",
        "    for i in range(1, nr_games + 1):\n",
        "        \n",
        "        if i%250 == 0: \n",
        "            average_mopt[i], average_mran[i] = computeTestM(player_q_learn)\n",
        "        \n",
        "        env.reset()\n",
        "\n",
        "        player_q_learn.epsilon = np.max([e_min, e_max * (1 - i / n)])\n",
        "\n",
        "        player_opt.set_player(j=i)\n",
        "        player_q_learn.set_player(j=i)\n",
        "\n",
        "        grid, _, _ = env.observe()\n",
        "\n",
        "        for j in range(9):\n",
        "                if env.current_player == player_opt.player:\n",
        "                    move = player_opt.act(grid)\n",
        "                    grid_after_player_opt, end, winner = env.step(move, print_grid=False)\n",
        "                    if not end: player_q_learn.tdUpdate(grid, grid_after_player_opt)\n",
        "                else:\n",
        "                    move = player_q_learn.act(grid)\n",
        "                    grid_after_player_q_learn, end, winner = env.step(move, print_grid=False)\n",
        "                if end:           \n",
        "                    reward = env.reward(player=player_q_learn.player)\n",
        "                    reward_every_250_games += reward\n",
        "                    player_q_learn.feedReward(reward, grid_after_player_q_learn)\n",
        "                    break\n",
        "                else:\n",
        "                    if env.current_player == player_opt.player: grid = grid_after_player_q_learn\n",
        "                    else: grid = grid_after_player_opt\n",
        "\n",
        "    average_mopt_array.append(average_mopt)\n",
        "    average_mran_array.append(average_mran)"
      ],
      "metadata": {
        "id": "CzDnHNtQtfZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(1, figsize=(15, 10))\n",
        "colors = ['orange', 'purple', 'green','red', 'pink']\n",
        "for i,n in enumerate(eps_opt):\n",
        "    plt.plot(list(average_mopt_array[i].keys()), list(average_mopt_array[i].values()), label=n, linestyle='-', color=colors[i])\n",
        "    plt.plot(list(average_mran_array[i].keys()), list(average_mran_array[i].values()), label=n, linestyle='--', color=colors[i])\n",
        "plt.title(r\"$M_{rand}$ (--) and $M_{opt}$ (-) for different values of $\\epsilon_{opt}$ with $n^*=5000$\")\n",
        "plt.legend(title='$\\epsilon_{opt}$ values')\n",
        "plt.xlabel(\"Number of games\")\n",
        "plt.ylabel('Mean reward')\n",
        "plt.grid()\n",
        "plt.savefig(save_dir + \"M_opt and M_rand epsilon.jpg\", dpi=400)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1DJmBp6USY8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7"
      ],
      "metadata": {
        "id": "xViWxWaK0iZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eps_opt = [0.2, 0.4, 0.6, 0.8]\n",
        "\n",
        "nr_games = 20000\n",
        "average_mopt_array = []\n",
        "average_mran_array = []\n",
        "\n",
        "for e_opt in eps_opt:\n",
        "\n",
        "    player_q1 = QlearningPlayer(epsilon=e_opt)\n",
        "    player_q2 = QlearningPlayer(epsilon=e_opt)\n",
        "    player_q1.Q_state_action = player_q2.Q_state_action\n",
        "\n",
        "    average_mopt = {}\n",
        "    average_mran = {}\n",
        "\n",
        "    for i in range(1, nr_games + 1):\n",
        "        \n",
        "        if i%250 == 0: \n",
        "          average_mopt[i], average_mran[i] = computeTestM(player_q1)\n",
        "        \n",
        "        env.reset()\n",
        "        player_q2.set_player(j=i)\n",
        "        player_q1.set_player(j=i+1)\n",
        "\n",
        "        grid, _, _ = env.observe()\n",
        "\n",
        "        for j in range(9):\n",
        "                if env.current_player == player_q2.player:\n",
        "                    move = player_q2.act(grid)\n",
        "                    grid_after_player_q2, end, winner = env.step(move, print_grid=False)\n",
        "                    if not end and j != 0: player_q1.tdUpdate(grid, grid_after_player_q2)\n",
        "                    \n",
        "                else:\n",
        "                    move = player_q1.act(grid)\n",
        "                    grid_after_player_q1, end, winner = env.step(move, print_grid=False)\n",
        "                    if not end and j != 0: player_q2.tdUpdate(grid, grid_after_player_q1)\n",
        "                if end: \n",
        "                            \n",
        "                    reward_q1 = env.reward(player = player_q1.player)\n",
        "                    player_q1.feedReward(reward_q1, grid_after_player_q1)\n",
        "\n",
        "                    reward_q2 = env.reward(player = player_q2.player)\n",
        "                    player_q2.feedReward(reward_q2, grid_after_player_q2) \n",
        "                    break\n",
        "                else:\n",
        "                    if env.current_player == player_q2.player: grid = grid_after_player_q1\n",
        "                    else: grid = grid_after_player_q2\n",
        "    average_mopt_array.append(average_mopt)\n",
        "    average_mran_array.append(average_mran)"
      ],
      "metadata": {
        "id": "Dy14p02dvmVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(1, figsize=(15, 10))\n",
        "colors = ['orange', 'purple', 'green','red', 'pink']\n",
        "for i,n in enumerate(eps_opt):\n",
        "    plt.plot(list(average_mopt_array[i].keys()), list(average_mopt_array[i].values()), label=n, linestyle='-', color=colors[i])\n",
        "    plt.plot(list(average_mran_array[i].keys()), list(average_mran_array[i].values()), label=n, linestyle='--', color=colors[i])\n",
        "plt.title(r\"$M_{rand}$ (--) and $M_{opt}$ (-) for different values of $\\epsilon$\")\n",
        "plt.legend(title='$\\epsilon$ values')\n",
        "plt.xlabel(\"Number of games\")\n",
        "plt.ylabel('Mean reward')\n",
        "plt.ylim((-1, 1))\n",
        "plt.grid()\n",
        "plt.savefig(save_dir + \"Learning by self-practice M_opt and M_rand fixed epsilon.jpg\", dpi=400)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wrmpLugY77d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8"
      ],
      "metadata": {
        "id": "G_zoDOTTbwhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nr_games = 20000\n",
        "n_array = np.linspace(1,20000,5,dtype=int)\n",
        "\n",
        "average_mopt_array_q8 = []\n",
        "average_mran_array_q8 = []\n",
        "\n",
        "for n in n_array:\n",
        "\n",
        "    player_q1 = QlearningPlayer(player = 'O')\n",
        "    player_q2 = QlearningPlayer(player = 'X')\n",
        "    \n",
        "    # both players use the same set of Q-values and update the same set of Q-values. \n",
        "    Q_state_action = {}\n",
        "    player_q1.Q_state_action = Q_state_action\n",
        "    player_q2.Q_state_action = Q_state_action\n",
        "\n",
        "    average_mopt = {}\n",
        "    average_mran = {}\n",
        "\n",
        "    for i in range(1, nr_games + 1):\n",
        "        \n",
        "        if i%250 == 0: \n",
        "            average_mopt[i], average_mran[i] = computeTestM(player_q1)\n",
        "        \n",
        "        env.reset()\n",
        "\n",
        "        # update epsilon\n",
        "        player_q1.epsilon = np.max([e_min, e_max*(1-i/n)])\n",
        "        player_q2.epsilon = np.max([e_min, e_max*(1-i/n)])\n",
        "        \n",
        "        #switch the 1st player after every game\n",
        "\n",
        "        player_q2.set_player(j=i)\n",
        "        player_q1.set_player(j=i+1)\n",
        "\n",
        "        grid, _, _ = env.observe()\n",
        "\n",
        "        for j in range(9):\n",
        "                if env.current_player == player_q2.player:\n",
        "                    move = player_q2.act(grid)\n",
        "                    grid_after_player_q2, end, winner = env.step(move, print_grid=False)\n",
        "                    if not end and j != 0: player_q1.tdUpdate(grid, grid_after_player_q2)\n",
        "                else:\n",
        "                    move = player_q1.act(grid)\n",
        "                    grid_after_player_q1, end, winner = env.step(move, print_grid=False)\n",
        "                    if not end and j != 0: player_q2.tdUpdate(grid, grid_after_player_q1)\n",
        "                if end:           \n",
        "                    reward_q1 = env.reward(player = player_q1.player)\n",
        "                    player_q1.feedReward(reward_q1, grid_after_player_q1)\n",
        "\n",
        "                    reward_q2 = env.reward(player = player_q2.player)\n",
        "                    player_q2.feedReward(reward_q2, grid_after_player_q2)\n",
        "\n",
        "                    break\n",
        "                else:\n",
        "                    #current_player has changed after step\n",
        "                    if env.current_player == player_q2.player: grid = grid_after_player_q1\n",
        "                    else: grid = grid_after_player_q2\n",
        "\n",
        "    average_mopt_array_q8.append(average_mopt)\n",
        "    average_mran_array_q8.append(average_mran)"
      ],
      "metadata": {
        "id": "1yIpknfmb0_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(1, figsize=(15, 10))\n",
        "colors = ['orange', 'purple', 'green','red', 'pink']\n",
        "for i,n in enumerate(n_array):\n",
        "    plt.plot(list(average_mopt_array_q8[i].keys()), list(average_mopt_array_q8[i].values()), label=n, linestyle='-', color=colors[i])\n",
        "    plt.plot(list(average_mran_array_q8[i].keys()), list(average_mran_array_q8[i].values()), label=n, linestyle='--', color=colors[i])\n",
        "plt.title(r\"$M_{rand}$ (--) and $M_{opt}$ (-) for different values of $n^*$\")\n",
        "plt.legend(title='$n^*$ values')\n",
        "plt.xlabel(\"Number of games\")\n",
        "plt.ylabel('Mean reward')\n",
        "plt.ylim((-1, 1))\n",
        "plt.grid()\n",
        "plt.savefig(os.getcwd() + \"\\\\plots_and_outputs\\\\plot_q8_2.jpg\", dpi=400)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lnEWqBbdcYIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10"
      ],
      "metadata": {
        "id": "TL88ovPN8Z1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grids = np.array([[[1.,0.,0.],[0.,-1.,0.],[0.,0.,0.]],\n",
        "                [[-1.,-1.,0.],[1.,0.,0.],[0.,1.,0.]],\n",
        "               [[-1.,0.,0.],[0.,-1.,0.],[1.,1.,0.]]])"
      ],
      "metadata": {
        "id": "g-eSQMmk8Y54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positions = np.array([[(i,j) for j in range(3)] for i in range(3)])\n",
        "positions = positions.reshape(-1,2)\n",
        "positions"
      ],
      "metadata": {
        "id": "HSn2q3yacmqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,3, figsize=(21,6))\n",
        "labels= np.array([[['X','',''],['','O',''],['','','',]],\n",
        "        [['O','O',''],['X','',''],['','X','']],\n",
        "        [['O','',''],['','O',''],['X','X','']]])\n",
        "\n",
        "for i,g in enumerate(grids):\n",
        "    aval = player_q1.empty(g)\n",
        "    qvalues = np.zeros((3,3))\n",
        "    for pos in aval:\n",
        "        next_grid = g.copy()\n",
        "        next_grid[pos] = 1\n",
        "        hash_next_grid = player_q1.hashGrid(next_grid)\n",
        "        qvalues[pos] = player_q1.Q_state_action.get(hash_next_grid,0)\n",
        "    print(qvalues)\n",
        "    sns.heatmap(qvalues,ax=ax[i], cmap=\"Blues\",  annot=labels[i], annot_kws={'fontsize': 25}, fmt='s')\n",
        "    ax[i].axis('off') \n",
        "\n",
        "plt.suptitle('Heatmap of Q-Values (player X) in 3 different states')\n",
        "plt.savefig(save_dir + \"q10.jpg\", dpi=400)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QYwzDfQScoa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Q-Learning"
      ],
      "metadata": {
        "id": "P1I2g2i9b8Dg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `Transition` - a named tuple representing a single transition in our environment. It essentially maps (state, action) pairs to their (next_state, reward) result, with the state being the screen difference image as described later on.\n",
        "\n",
        "- `ReplayMemory` - a cyclic buffer of bounded size that holds the transitions observed recently. It also implements a `.sample()` method for selecting a random batch of transitions for training."
      ],
      "metadata": {
        "id": "5eUWCIimWk1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([],maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, next_state, reward):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        state = torch.tensor(state, device=device).unsqueeze(0)\n",
        "        action = torch.tensor([action], device=device)\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        self.memory.append(Transition(state, action, next_state, reward))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "rIKhghKKWR3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(2*3*3, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 9)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns Q-value of the corresponding action at state x_t\n",
        "    def forward(self, x_t):\n",
        "      x_t = x_t.to(device).type(torch.cuda.FloatTensor)\n",
        "      x_t = F.relu(self.fc1(x_t.view(-1, 2*3*3)))\n",
        "      x_t = F.relu(self.fc2(x_t))\n",
        "      return self.fc3(x_t) "
      ],
      "metadata": {
        "id": "v4wxcwh8WkPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepQlearningPlayer:\n",
        "    '''\n",
        "    Description:\n",
        "        A class to implement a player in Tic-tac-toe based on the Deep-Q-learning algorithm\n",
        "        combined with epsilon-greedy policy.\n",
        "    '''\n",
        "    def __init__(self, player='X',epsilon=0.05):\n",
        "        self.discount_factor = 0.99\n",
        "        self.alpha = 5e-4\n",
        "        self.capacity = 10000\n",
        "        self.gamma = 0.999\n",
        "        self.epsilon = epsilon\n",
        "        self.player = player \n",
        "        self.batch_size = 64\n",
        "\n",
        "        self.policy_net = DQN().to(device)\n",
        "        self.target_net = DQN().to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.alpha)\n",
        "        self.memory = ReplayMemory(self.capacity)\n",
        "\n",
        "        self.loss_every_250_games = 0\n",
        "\n",
        "\n",
        "    def set_player(self, player = 'O', j=-1):\n",
        "        self.player = player\n",
        "        if j != -1:\n",
        "            self.player = 'O' if j % 2 == 0 else 'X' #the opposite is done in OptimalPlayer\n",
        "\n",
        "\n",
        "    def randomMove(self, x_t):\n",
        "        \"\"\" Choose a random move from the available ones\"\"\"\n",
        "        avail = []\n",
        "        for i in range(9):\n",
        "            pos = (int(i/3), i % 3)\n",
        "            if x_t[0][pos] == 0 and x_t[1][pos] == 0:\n",
        "                avail.append(i)\n",
        "        \n",
        "        move = random.randint(0, len(avail) - 1)\n",
        "        return move\n",
        "\n",
        "\n",
        "    def highestQMove(self, x_t):\n",
        "        \"\"\"\n",
        "        Choose the action which has the highest Q-value predicted by the policy net.\n",
        "        \"\"\"\n",
        "        x_t = torch.tensor(x_t, dtype=torch.float64)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "              # t.max(1) will return largest column value of each row.\n",
        "              # second column on max result is index of where max element was\n",
        "              # found, so we pick action with the larger expected reward.\n",
        "              move = self.policy_net(x_t).max(1)[1].item()\n",
        "\n",
        "        return move\n",
        "    \n",
        "\n",
        "    def act(self, x_t):\n",
        "        \"\"\"\n",
        "        A int between 0 and 8 is returned that represents the move on the grid.\n",
        "        \"\"\"\n",
        "\n",
        "        # choose action according to epsilon-greedy policy\n",
        "        # i.e. whether to move in random or not\n",
        "        if random.random() < self.epsilon: action = self.randomMove(x_t)\n",
        "        else: action = self.highestQMove(x_t)\n",
        "        \n",
        "        return action\n",
        "\n",
        "    \n",
        "    def optimize_model(self, add_to_total_loss):\n",
        "\n",
        "      if len(self.memory) < self.batch_size:\n",
        "          return\n",
        "\n",
        "\n",
        "      #sample random mini-batch of transitions from the replay buffer  \n",
        "      transitions = self.memory.sample(self.batch_size)\n",
        "\n",
        "      # Transpose the batch. This converts batch-array of Transitions\n",
        "      # to Transition of batch-arrays.\n",
        "      batch = Transition(*zip(*transitions))\n",
        "\n",
        "      # Compute a mask of non-final states and concatenate the batch elements\n",
        "      # (a final state would've been the one after which simulation ended)\n",
        "      non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                            batch.next_state)), device=device, dtype=torch.bool)\n",
        "      \n",
        "      non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "      \n",
        "      state_batch = torch.cat(batch.state)\n",
        "      action_batch = torch.cat(batch.action)\n",
        "      reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "\n",
        "      # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "      # columns of actions taken. These are the actions which would've been taken\n",
        "      # for each batch state according to policy_net\n",
        "      state_action_values = self.policy_net(state_batch).gather(1, action_batch.view(-1, 1))\n",
        "\n",
        "      # Compute V(s_{t+1}) for all next states.\n",
        "      # Expected values of actions for non_final_next_states are computed based\n",
        "      # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "      # This is merged based on the mask, such that we'll have either the expected\n",
        "      # state value or 0 in case the state was final.\n",
        "      next_state_values = torch.zeros(self.batch_size, device=device)\n",
        "      next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
        "      # Compute the expected Q values\n",
        "      expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
        "\n",
        "      # Compute Huber loss\n",
        "      criterion = nn.SmoothL1Loss()\n",
        "      loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "      if add_to_total_loss: self.loss_every_250_games += loss\n",
        "\n",
        "      # Optimize the model\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      for param in self.policy_net.parameters():\n",
        "          param.grad.data.clamp_(-1, 1)\n",
        "      self.optimizer.step()\n",
        "\n",
        "    def optimize_latest_transition(self, add_to_total_loss, latest_transition):\n",
        "      \"\"\"Train but without the replay buffer and with a batch size of 1.\n",
        "        At every step, update the network by using only the latest transition.\"\"\"\n",
        "      \n",
        "      if latest_transition.next_state != None:\n",
        "        next_state_value = self.target_net(latest_transition.next_state).max(1)[0].detach().item()\n",
        "      else: next_state_value = torch.zeros(1, device=device)\n",
        "   \n",
        "      action_tensor = torch.tensor([latest_transition.action], device=device)\n",
        "      state_action_value = self.policy_net(torch.tensor(latest_transition.state, device=device)).gather(1, action_tensor.view(-1, 1))[0]\n",
        "      \n",
        "      expected_state_action_value = (next_state_value * self.gamma) + torch.tensor([latest_transition.reward], device=device)\n",
        "\n",
        "      # Compute Huber loss\n",
        "      criterion = nn.SmoothL1Loss()\n",
        "      loss = criterion(state_action_value, expected_state_action_value)\n",
        "\n",
        "      if add_to_total_loss: self.loss_every_250_games += loss\n",
        "\n",
        "      # Optimize the model\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      for param in self.policy_net.parameters():\n",
        "          param.grad.data.clamp_(-1, 1)\n",
        "      self.optimizer.step()"
      ],
      "metadata": {
        "id": "ER3WRVIsspL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = TictactoeEnv()"
      ],
      "metadata": {
        "id": "_4F3tCFMpFPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 11"
      ],
      "metadata": {
        "id": "CqZgzs7PeHof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eps = 0.1\n",
        "\n",
        "player_opt = OptimalPlayer(epsilon=0.5, player = 'X')\n",
        "player_deep_q_learn = DeepQlearningPlayer(player = 'O',epsilon=eps)\n",
        "\n",
        "nr_games = 20000\n",
        "reward_every_250_games = 0\n",
        "\n",
        "average_rewards = {}\n",
        "average_loss = {}\n",
        "update_target_network = 500\n",
        "end = False\n",
        "\n",
        "for i in range(1, nr_games + 1):\n",
        "  # Update the target network, copying all weights and biases in DQN\n",
        "  if i % update_target_network == 0:\n",
        "    player_deep_q_learn.target_net.load_state_dict(player_deep_q_learn.policy_net.state_dict())\n",
        "\n",
        "  if i % 250 == 0: \n",
        "      average_rewards[i] = reward_every_250_games / 250\n",
        "      average_loss[i] = player_deep_q_learn.loss_every_250_games / 250\n",
        "      reward_every_250_games = 0\n",
        "      player_deep_q_learn.loss_every_250_games = 0\n",
        "  \n",
        "  env.reset()\n",
        "  #switch the 1st player after every game\n",
        "  player_opt.set_player(j=i)\n",
        "  player_deep_q_learn.set_player(j=i)\n",
        "  \n",
        "  grid, _, __ = env.observe()\n",
        "  x_t = np.stack([grid, grid])\n",
        "  \n",
        "  for j in range(9):\n",
        "\n",
        "    if env.current_player == player_opt.player:\n",
        "      move = player_opt.act(grid)\n",
        "      x_t[1][move] = 1 #update state\n",
        "\n",
        "      grid_after_player_opt, end, winner = env.step(move, print_grid=False)\n",
        "      #if j=0 here, then player_opt starts the game, so we don't have a current_state yet\n",
        "      if not end and j != 0:\n",
        "        next_state = torch.tensor(x_t.copy(), device=device).unsqueeze(0)\n",
        "        reward = 0\n",
        "        player_deep_q_learn.memory.push(current_state, move_deep, next_state, reward)\n",
        "        player_deep_q_learn.optimize_model(add_to_total_loss=False) #optimize model but game not ended, so don't add to total loss\n",
        "      if end: reward = env.reward(player=player_deep_q_learn.player)\n",
        "    \n",
        "    else:\n",
        "      current_state = x_t.copy() \n",
        "      move_deep = player_deep_q_learn.act(current_state)\n",
        "      move_deep_pos = (int(move_deep/3), move_deep % 3)\n",
        "\n",
        "      if current_state[0][move_deep_pos] == 1 or current_state[1][move_deep_pos] == 1: #unavailable position\n",
        "        reward = -1\n",
        "        end = True\n",
        "      else:\n",
        "        x_t[0][move_deep_pos] = 1\n",
        "        grid_after_player_deep_q_learn, end, winner = env.step(move_deep_pos, print_grid=False)\n",
        "        if end: reward = env.reward(player=player_deep_q_learn.player)\n",
        "        \n",
        "    if end:\n",
        "      \n",
        "      next_state = None\n",
        "      reward_every_250_games += reward\n",
        "      player_deep_q_learn.memory.push(current_state, move_deep, next_state, reward)\n",
        "      player_deep_q_learn.optimize_model(add_to_total_loss=True)#game ended, so add to total loss\n",
        "      env.reset()\n",
        "      break\n",
        "\n",
        "    else:\n",
        "        #current_player has changed after step\n",
        "        if env.current_player == player_opt.player: grid = grid_after_player_deep_q_learn\n",
        "        else: grid = grid_after_player_opt      "
      ],
      "metadata": {
        "id": "w07XfjolEpjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2, figsize=(10, 7))\n",
        "fig.suptitle(\"DQN agent with $\\epsilon=0.1$ vs Opt(0.5)\")\n",
        "axs[0].plot(list(average_rewards.keys()), list(average_rewards.values()))\n",
        "axs[0].set_xlabel(\"Number of games played\")\n",
        "axs[0].set_ylabel(\"Average reward\")\n",
        "axs[0].set_ylim((-1, 1))\n",
        "axs[0].grid()\n",
        "axs[1].plot(list(average_loss.keys()), [x.item() for x in list(average_loss.values())])\n",
        "axs[1].set_xlabel(\"Number of games played\")\n",
        "axs[1].set_ylabel(\"Average loss \")\n",
        "axs[1].grid()\n",
        "plt.savefig(save_dir + \"DQN vs Opt(0.5).jpg\", dpi=400)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sOJyOUfHT-7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 12\n"
      ],
      "metadata": {
        "id": "SYdGskRiepG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eps = 0.1\n",
        "\n",
        "player_opt = OptimalPlayer(epsilon=0.5, player = 'X')\n",
        "player_deep_q_learn = DeepQlearningPlayer(player = 'O',epsilon=eps)\n",
        "\n",
        "nr_games = 20000\n",
        "reward_every_250_games = 0\n",
        "\n",
        "average_rewards = {}\n",
        "average_loss = {}\n",
        "update_target_network = 500\n",
        "end = False\n",
        "\n",
        "for i in range(1, nr_games + 1):\n",
        "  # Update the target network, copying all weights and biases in DQN\n",
        "  if i % update_target_network == 0:\n",
        "    player_deep_q_learn.target_net.load_state_dict(player_deep_q_learn.policy_net.state_dict())\n",
        "\n",
        "  if i % 250 == 0: \n",
        "      average_rewards[i] = reward_every_250_games / 250\n",
        "      average_loss[i] = player_deep_q_learn.loss_every_250_games / 250\n",
        "      reward_every_250_games = 0\n",
        "      player_deep_q_learn.loss_every_250_games = 0\n",
        "  \n",
        "  env.reset()\n",
        "\n",
        "  #switch the 1st player after every game\n",
        "  player_opt.set_player(j=i)\n",
        "  player_deep_q_learn.set_player(j=i)\n",
        "  \n",
        "  grid, _, __ = env.observe()\n",
        "  x_t = np.stack([grid, grid])\n",
        "  \n",
        "  for j in range(9):\n",
        "\n",
        "    if env.current_player == player_opt.player:\n",
        "      move = player_opt.act(grid)\n",
        "      x_t[1][move] = 1 \n",
        "      grid_after_player_opt, end, winner = env.step(move, print_grid=False)\n",
        "      if not end and j != 0:\n",
        "        next_state = torch.tensor(x_t.copy(), device=device).unsqueeze(0)\n",
        "        reward = 0\n",
        "        transition = Transition(current_state, move_deep, next_state, reward)\n",
        "        player_deep_q_learn.optimize_latest_transition(False, transition)\n",
        "      if end: reward = env.reward(player=player_deep_q_learn.player)\n",
        "    \n",
        "    else:\n",
        "      current_state = x_t.copy() \n",
        "      move_deep = player_deep_q_learn.act(current_state)\n",
        "      move_deep_pos = (int(move_deep/3), move_deep % 3)\n",
        "\n",
        "      if current_state[0][move_deep_pos] == 1 or current_state[1][move_deep_pos] == 1: #unavailable position\n",
        "        reward = -1\n",
        "        end = True\n",
        "      else:\n",
        "        x_t[0][move_deep_pos] = 1\n",
        "        grid_after_player_deep_q_learn, end, winner = env.step(move_deep_pos, print_grid=False)\n",
        "        if end: reward = env.reward(player=player_deep_q_learn.player)\n",
        "        \n",
        "    if end:\n",
        "      next_state = None\n",
        "      reward_every_250_games += reward\n",
        "      transition = Transition(current_state, move_deep, next_state, reward)\n",
        "      player_deep_q_learn.optimize_latest_transition(True, transition)\n",
        "      env.reset()\n",
        "      break\n",
        "\n",
        "    else:\n",
        "        #current_player has changed after step\n",
        "        if env.current_player == player_opt.player: grid = grid_after_player_deep_q_learn\n",
        "        else: grid = grid_after_player_opt      "
      ],
      "metadata": {
        "id": "q2PPfUPjW1h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2, figsize=(10, 7))\n",
        "fig.suptitle(\"DQN agent with $\\epsilon=0.1$, no replay buffer vs Opt(0.5)\")\n",
        "axs[0].plot(list(average_rewards.keys()), list(average_rewards.values()))\n",
        "axs[0].set_xlabel(\"Number of games played\")\n",
        "axs[0].set_ylabel(\"Average reward\")\n",
        "axs[0].set_ylim((-1, 1))\n",
        "axs[0].grid()\n",
        "axs[1].plot(list(average_loss.keys()), [x.item() for x in list(average_loss.values())])\n",
        "axs[1].set_xlabel(\"Number of games played\")\n",
        "axs[1].set_ylabel(\"Average loss \")\n",
        "axs[1].grid()\n",
        "plt.savefig(save_dir + \"DQN no buffer vs Opt(0.5).jpg\", dpi=400)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J4ppIEbT7ZjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 13"
      ],
      "metadata": {
        "id": "6OeWNhkR7k84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_game(player_deep_q_learn, player_opt, i):\n",
        "\n",
        "    env.reset()\n",
        "    player_opt.set_player(j=i); player_deep_q_learn.set_player(j=i)\n",
        "    grid, _, _ =  env.observe()\n",
        "    x_t = np.stack([grid, grid])\n",
        "    reward = 0\n",
        "\n",
        "    for j in range(9):\n",
        "        if env.current_player == player_opt.player:\n",
        "            move = player_opt.act(grid)\n",
        "            grid, end, _ = env.step(move, print_grid=False)\n",
        "            x_t[1][move] = 1\n",
        "        else:\n",
        "          current_state = x_t.copy() \n",
        "          move_deep = player_deep_q_learn.act(current_state)\n",
        "          move_deep_pos = (int(move_deep/3), move_deep % 3)\n",
        "\n",
        "          if current_state[0][move_deep_pos] == 1 or current_state[1][move_deep_pos] == 1: \n",
        "            end = True\n",
        "            reward = -1\n",
        "          else:\n",
        "            x_t[0][move_deep_pos] = 1\n",
        "            grid, end, _ = env.step(move_deep_pos, print_grid=False)\n",
        "\n",
        "        if end:\n",
        "            if reward != -1: reward = env.reward(player=player_deep_q_learn.player)\n",
        "            break\n",
        "\n",
        "    return reward"
      ],
      "metadata": {
        "id": "QgoH4QcfCoTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def computeTestM(player_deep_q_learn):\n",
        "    N = 500\n",
        "    opt, ran = 0, 0\n",
        "    player_opt = OptimalPlayer()\n",
        "    player_opt.epsilon, player_deep_q_learn.epsilon = 0, 0\n",
        "    \n",
        "    # First we compute Mopt\n",
        "    for i in range(N):\n",
        "        opt += one_game(player_deep_q_learn, player_opt, i)\n",
        "        \n",
        "    # Then we compute Mrand\n",
        "    player_opt.epsilon = 1\n",
        "    for i in range(N):\n",
        "        ran += one_game(player_deep_q_learn, player_opt, i)    \n",
        "    \n",
        "    return opt / N, ran / N"
      ],
      "metadata": {
        "id": "8jtJFAlqCk1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e_max, e_min = 0.8, 0.1\n",
        "n_expl = np.linspace(1, 20000, 5, dtype=int)\n",
        "\n",
        "average_mopt_array = []\n",
        "average_mran_array = []\n",
        "\n",
        "for n in n_expl:\n",
        "  average_mopt = {}\n",
        "  average_mran = {}\n",
        "\n",
        "  player_opt = OptimalPlayer(epsilon=0.5)\n",
        "  player_deep_q_learn = DeepQlearningPlayer()\n",
        "\n",
        "  nr_games = 20000\n",
        "  update_target_network = 500\n",
        "  end = False\n",
        "\n",
        "  for i in range(1, nr_games + 1):\n",
        "    player_deep_q_learn.epsilon = np.max([e_min, e_max * (1 - i / n)])\n",
        "\n",
        "    if i % update_target_network == 0:\n",
        "      player_deep_q_learn.target_net.load_state_dict(player_deep_q_learn.policy_net.state_dict())\n",
        "\n",
        "    if i%250 == 0: \n",
        "          average_mopt[i], average_mran[i] = computeTestM(player_deep_q_learn)\n",
        "    \n",
        "    env.reset()\n",
        "    player_opt.set_player(j=i)\n",
        "    player_deep_q_learn.set_player(j=i)\n",
        "    \n",
        "    grid, _, __ = env.observe()\n",
        "    x_t = np.stack([grid, grid])\n",
        "    \n",
        "    for j in range(9):\n",
        "\n",
        "      if env.current_player == player_opt.player:\n",
        "        move = player_opt.act(grid)\n",
        "        x_t[1][move] = 1\n",
        "        grid_after_player_opt, end, winner = env.step(move, print_grid=False)\n",
        "        if not end and j != 0:\n",
        "          next_state = torch.tensor(x_t.copy(), device=device).unsqueeze(0)\n",
        "          reward = 0\n",
        "          player_deep_q_learn.memory.push(current_state, move_deep, next_state, reward)\n",
        "          player_deep_q_learn.optimize_model(add_to_total_loss=False)\n",
        "        if end: reward = env.reward(player=player_deep_q_learn.player)\n",
        "      \n",
        "      else:\n",
        "        current_state = x_t.copy() \n",
        "        move_deep = player_deep_q_learn.act(current_state)\n",
        "        move_deep_pos = (int(move_deep/3), move_deep % 3)\n",
        "\n",
        "        if current_state[0][move_deep_pos] == 1 or current_state[1][move_deep_pos] == 1:\n",
        "          reward = -1\n",
        "          end = True\n",
        "        else:\n",
        "          x_t[0][move_deep_pos] = 1\n",
        "          grid_after_player_deep_q_learn, end, winner = env.step(move_deep_pos, print_grid=False)\n",
        "          if end: reward = env.reward(player=player_deep_q_learn.player)\n",
        "          \n",
        "      if end:\n",
        "        next_state = None\n",
        "        reward_every_250_games += reward\n",
        "        player_deep_q_learn.memory.push(current_state, move_deep, next_state, reward)\n",
        "        player_deep_q_learn.optimize_model(add_to_total_loss=True)\n",
        "        env.reset()\n",
        "        break\n",
        "\n",
        "      else:\n",
        "          if env.current_player == player_opt.player: grid = grid_after_player_deep_q_learn\n",
        "          else: grid = grid_after_player_opt\n",
        "  average_mopt_array.append(average_mopt)\n",
        "  average_mran_array.append(average_mran)      "
      ],
      "metadata": {
        "id": "9wXRzn9QAHdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(1, figsize=(15, 10))\n",
        "colors = ['orange', 'purple', 'green','red', 'pink']\n",
        "for i,n in enumerate(n_expl):\n",
        "    plt.plot(list(average_mopt_array[i].keys()), list(average_mopt_array[i].values()), label=n, linestyle='-', color=colors[i])\n",
        "    plt.plot(list(average_mran_array[i].keys()), list(average_mran_array[i].values()), label=n, linestyle='--', color=colors[i])\n",
        "plt.title(r\"$M_{rand}$ (--) and $M_{opt}$ (-) for different values of $n^*$\")\n",
        "plt.legend(title='$n^*$ values')\n",
        "plt.xlabel(\"Number of games\")\n",
        "plt.ylabel('Mean reward')\n",
        "plt.ylim((-1, 1))\n",
        "plt.grid()\n",
        "plt.savefig(save_dir + \"DQN vs Opt(0.5) M_opt and M_rand n*.jpg\", dpi=400)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3C-i_jAeGOxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 14"
      ],
      "metadata": {
        "id": "LBXk98fEce_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 5000\n",
        "e_max, e_min = 0.8, 0.1\n",
        "eps_opt = [0.2, 0.4, 0.6, 0.8]\n",
        "\n",
        "average_mopt_array = []\n",
        "average_mran_array = []\n",
        "\n",
        "for eps in eps_opt:\n",
        "  average_mopt = {}\n",
        "  average_mran = {}\n",
        "\n",
        "  player_opt = OptimalPlayer(epsilon=eps)\n",
        "  player_deep_q_learn = DeepQlearningPlayer()\n",
        "\n",
        "  nr_games = 20000\n",
        "  update_target_network = 500\n",
        "  end = False\n",
        "\n",
        "  for i in range(1, nr_games + 1):\n",
        "    player_deep_q_learn.epsilon = np.max([e_min, e_max * (1 - i / n)])\n",
        "\n",
        "    if i % update_target_network == 0:\n",
        "      player_deep_q_learn.target_net.load_state_dict(player_deep_q_learn.policy_net.state_dict())\n",
        "\n",
        "    if i%250 == 0: \n",
        "          average_mopt[i], average_mran[i] = computeTestM(player_deep_q_learn)\n",
        "    \n",
        "    env.reset()\n",
        "    player_opt.set_player(j=i)\n",
        "    player_deep_q_learn.set_player(j=i)\n",
        "    \n",
        "    grid, _, __ = env.observe()\n",
        "    x_t = np.stack([grid, grid])\n",
        "    \n",
        "    for j in range(9):\n",
        "\n",
        "      if env.current_player == player_opt.player:\n",
        "        move = player_opt.act(grid)\n",
        "        x_t[1][move] = 1\n",
        "        grid_after_player_opt, end, winner = env.step(move, print_grid=False)\n",
        "        if not end and j != 0:\n",
        "          next_state = torch.tensor(x_t.copy(), device=device).unsqueeze(0)\n",
        "          reward = 0\n",
        "          player_deep_q_learn.memory.push(current_state, move_deep, next_state, reward)\n",
        "          player_deep_q_learn.optimize_model(add_to_total_loss=False)\n",
        "        if end: reward = env.reward(player=player_deep_q_learn.player)\n",
        "      \n",
        "      else:\n",
        "        current_state = x_t.copy() \n",
        "        move_deep = player_deep_q_learn.act(current_state)\n",
        "        move_deep_pos = (int(move_deep / 3), move_deep % 3)\n",
        "\n",
        "        if current_state[0][move_deep_pos] == 1 or current_state[1][move_deep_pos] == 1:\n",
        "          reward = -1\n",
        "          end = True\n",
        "        else:\n",
        "          x_t[0][move_deep_pos] = 1\n",
        "          grid_after_player_deep_q_learn, end, winner = env.step(move_deep_pos, print_grid=False)\n",
        "          if end: reward = env.reward(player=player_deep_q_learn.player)\n",
        "          \n",
        "      if end:\n",
        "        next_state = None\n",
        "        reward_every_250_games += reward\n",
        "        player_deep_q_learn.memory.push(current_state, move_deep, next_state, reward)\n",
        "        player_deep_q_learn.optimize_model(add_to_total_loss=True)\n",
        "        env.reset()\n",
        "        break\n",
        "\n",
        "      else:\n",
        "          if env.current_player == player_opt.player: grid = grid_after_player_deep_q_learn\n",
        "          else: grid = grid_after_player_opt\n",
        "  average_mopt_array.append(average_mopt)\n",
        "  average_mran_array.append(average_mran)      "
      ],
      "metadata": {
        "id": "UHdOTSbAHITl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(1, figsize=(15, 10))\n",
        "colors = ['orange', 'purple', 'green','red', 'pink']\n",
        "for i,n in enumerate(eps_opt):\n",
        "    plt.plot(list(average_mopt_array[i].keys()), list(average_mopt_array[i].values()), label=n, linestyle='-', color=colors[i])\n",
        "    plt.plot(list(average_mran_array[i].keys()), list(average_mran_array[i].values()), label=n, linestyle='--', color=colors[i])\n",
        "plt.title(r\"$M_{rand}$ (--) and $M_{opt}$ (-) for different values of $\\epsilon$ and $n^*=5000$\")\n",
        "plt.legend(title='$\\epsilon$ values')\n",
        "plt.xlabel(\"Number of games\")\n",
        "plt.ylabel('Mean reward')\n",
        "plt.ylim((-1, 1))\n",
        "plt.grid()\n",
        "plt.savefig(save_dir + \"DQN vs Opt(eps_opt) M_opt and M_rand fixed n*.jpg\", dpi=400)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wDSsT9eZeUz6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}